---
# Source: enkryptai-stack/charts/openfga/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openfga
  labels:
    helm.sh/chart: openfga-0.2.26
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.8.9"
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/auth/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-supabase-auth
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/kong/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-supabase-kong
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/meta/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-supabase-meta
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/minio/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: minio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/rest/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-supabase-rest
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/storage/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: supabase-storage
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/studio/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-supabase-studio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: enkryptai-stack/charts/supabase/templates/secrets/analytics.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-supabase-analytics
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
---
# Source: enkryptai-stack/charts/frontend/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend
  labels:
    helm.sh/chart: frontend-1.0.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
data:
  NEXT_PUBLIC_URL: "app.anishs.xyz"
---
# Source: enkryptai-stack/charts/gateway-kong/templates/fluent-bit-config-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
data:
  fluent-bit.conf: |
    [SERVICE]
      flush        1
      daemon       Off
      log_level    Off
      parsers_file parsers.conf
      plugins_file plugins.conf
      http_server  On
      http_listen  0.0.0.0
      http_port    2020
      Hot_Reload   On
      Health_Check On
      HC_Errors_Count 5
      HC_Retry_Failure_Count 5
      HC_Period 5
      storage.metrics on
      storage.path /tmp/storage
      storage.sync full
      storage.checksum on
      storage.max_chunks_up 128
      storage.backlog.mem_limit 100M
      storage.delete_irrecoverable_chunks On

    [INPUT]
      name http
      Alias input-admin
      storage.type filesystem
      listen 0.0.0.0
      port 5043
      Buffer_Max_Size 500MB
      buffer_chunk_size 5MB
      successful_response_code 201
      tag admin

    [INPUT]
      name http
      Alias input-guardrails
      storage.type filesystem
      listen 0.0.0.0
      port 5044
      Buffer_Max_Size 500MB
      buffer_chunk_size 5MB
      successful_response_code 201
      tag guardrails

    [INPUT]
      name http
      Alias input-red-teaming
      storage.type filesystem
      listen 0.0.0.0
      port 5045
      Buffer_Max_Size 500MB
      buffer_chunk_size 5MB
      successful_response_code 201
      tag red-teaming

    [INPUT]
      name http
      Alias input-leaderboard
      storage.type filesystem
      listen 0.0.0.0
      port 5046
      Buffer_Max_Size 500MB
      buffer_chunk_size 5MB
      successful_response_code 201
      tag leaderboard

    [INPUT]
      name http
      Alias input-ai-proxy
      storage.type filesystem
      listen 0.0.0.0
      port 5047
      Buffer_Max_Size 500MB
      buffer_chunk_size 5MB
      successful_response_code 201
      tag ai-proxy


    [OUTPUT]
      Name  es
      Alias output-admin
      Match admin
      Host  ${ELASTIC_HOST}
      Port  ${ELASTIC_PORT}
      HTTP_User ${ELASTIC_USERNAME}
      HTTP_Passwd ${ELASTIC_PASSWORD}
      Index admin
      Buffer_Size 100M
      Retry_Limit  3
      Suppress_Type_Name On
      # Logstash_Format true
      # Logstash_Prefix admin
      Generate_ID on
      tls On
      tls.verify  off
      # tls.ca_file /opt/certs/root-ca.pem

    [OUTPUT]
      Name  es
      Alias output-guardrails
      Match guardrails
      Host  ${ELASTIC_HOST}
      Port  ${ELASTIC_PORT}
      HTTP_User ${ELASTIC_USERNAME}
      HTTP_Passwd ${ELASTIC_PASSWORD}
      Index guardrails
      Buffer_Size 100M
      Retry_Limit  3
      Suppress_Type_Name On
      # Logstash_Format true
      # Logstash_Prefix guardrails
      Generate_ID on
      tls On
      tls.verify  off
      # tls.ca_file /opt/certs/root-ca.pem

    [OUTPUT]
      Name  es
      Alias output-red-teaming
      Match red-teaming
      Host  ${ELASTIC_HOST}
      Port  ${ELASTIC_PORT}
      HTTP_User ${ELASTIC_USERNAME}
      HTTP_Passwd ${ELASTIC_PASSWORD}
      Index red_teaming
      Buffer_Size 100M
      Retry_Limit  3
      Suppress_Type_Name On
      # Logstash_Format true
      # Logstash_Prefix red_teaming
      Generate_ID on
      tls On
      tls.verify  off
      # tls.ca_file /opt/certs/root-ca.pem

    [OUTPUT]
      Name  es
      Alias output-leaderboard
      Match leaderboard
      Host  ${ELASTIC_HOST}
      Port  ${ELASTIC_PORT}
      HTTP_User ${ELASTIC_USERNAME}
      HTTP_Passwd ${ELASTIC_PASSWORD}
      Index leaderboard
      Buffer_Size 100M
      Retry_Limit  3
      Suppress_Type_Name On
      # Logstash_Format true
      # Logstash_Prefix leaderboard
      Generate_ID on
      tls On
      tls.verify  off
      # tls.ca_file /opt/certs/root-ca.pem

    [OUTPUT]
      Name  es
      Alias output-ai-proxy
      Match ai-proxy
      Host  ${ELASTIC_HOST}
      Port  ${ELASTIC_PORT}
      HTTP_User ${ELASTIC_USERNAME}
      HTTP_Passwd ${ELASTIC_PASSWORD}
      Index ai-proxy
      Buffer_Size 100M
      Retry_Limit  3
      Suppress_Type_Name On
      # Logstash_Format true
      # Logstash_Prefix ai-proxy
      Generate_ID on
      tls On
      tls.verify  off
      # tls.ca_file /opt/certs/root-ca.pem

  parsers.conf: |
    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On
---
# Source: enkryptai-stack/charts/supabase/templates/kong/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-supabase-kong
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  wrapper.sh: |
    #!/bin/bash

    set -euo pipefail

    echo "Replacing env placeholders of /usr/local/kong/kong.yml"

    sed \
    -e "s/\${SUPABASE_ANON_KEY}/${SUPABASE_ANON_KEY}/" \
    -e "s/\${SUPABASE_SERVICE_KEY}/${SUPABASE_SERVICE_KEY}/" \
    -e "s/\${DASHBOARD_USERNAME}/${DASHBOARD_USERNAME}/" \
    -e "s/\${DASHBOARD_PASSWORD}/${DASHBOARD_PASSWORD}/" \
    /usr/local/kong/template.yml \
    > /usr/local/kong/kong.yml

    exec /docker-entrypoint.sh kong docker-start
  template.yml: |
    _format_version: '2.1'
    _transform: true

    consumers:
      - username: DASHBOARD
      - username: anon
        keyauth_credentials:
            - key: ${SUPABASE_ANON_KEY}
      - username: service_role
        keyauth_credentials:
            - key: ${SUPABASE_SERVICE_KEY}
    acls:
      - consumer: anon
        group: anon
      - consumer: service_role
        group: admin
    basicauth_credentials:
      - consumer: DASHBOARD
        username: ${DASHBOARD_USERNAME}
        password: ${DASHBOARD_PASSWORD}
    services:
      - name: auth-v1-open
        url: http://release-name-supabase-auth:9999/verify
        routes:
          - name: auth-v1-open
            strip_path: true
            paths:
              - /auth/v1/verify
        plugins:
          - name: cors
      - name: auth-v1-open-callback
        url: http://release-name-supabase-auth:9999/callback
        routes:
          - name: auth-v1-open-callback
            strip_path: true
            paths:
              - /auth/v1/callback
        plugins:
          - name: cors
      - name: auth-v1-open-authorize
        url: http://release-name-supabase-auth:9999/authorize
        routes:
          - name: auth-v1-open-authorize
            strip_path: true
            paths:
              - /auth/v1/authorize
        plugins:
          - name: cors
      - name: auth-v1
        _comment: "GoTrue: /auth/v1/* -> http://release-name-supabase-auth:9999/*"
        url: http://release-name-supabase-auth:9999
        routes:
          - name: auth-v1-all
            strip_path: true
            paths:
              - /auth/v1/
        plugins:
          - name: cors
          - name: key-auth
            config:
              hide_credentials: false
          - name: acl
            config:
              hide_groups_header: true
              allow:
                - admin
                - anon
      - name: rest-v1
        _comment: "PostgREST: /rest/v1/* -> http://release-name-supabase-rest:3000/*"
        url: http://release-name-supabase-rest:3000/
        routes:
          - name: rest-v1-all
            strip_path: true
            paths:
              - /rest/v1/
        plugins:
          - name: cors
          - name: key-auth
            config:
              hide_credentials: true
          - name: acl
            config:
              hide_groups_header: true
              allow:
                - admin
                - anon
      - name: graphql-v1
        _comment: 'PostgREST: /graphql/v1/* -> http://release-name-supabase-rest:3000/rpc/graphql'
        url: http://release-name-supabase-rest:3000/rpc/graphql
        routes:
          - name: graphql-v1-all
            strip_path: true
            paths:
              - /graphql/v1
        plugins:
          - name: cors
          - name: key-auth
            config:
              hide_credentials: true
          - name: request-transformer
            config:
              add:
                headers:
                  - Content-Profile:graphql_public
          - name: acl
            config:
              hide_groups_header: true
              allow:
                - admin
                - anon
      - name: storage-v1
        _comment: "Storage: /storage/v1/* -> http://supabase-storage:5000/*"
        url: http://supabase-storage:5000/
        routes:
          - name: storage-v1-all
            strip_path: true
            paths:
              - /storage/v1/
        plugins:
          - name: cors
      - name: meta
        _comment: "pg-meta: /pg/* -> http://release-name-supabase-meta:8080/*"
        url: http://release-name-supabase-meta:8080/
        routes:
          - name: meta-all
            strip_path: true
            paths:
              - /pg/
        plugins:
          - name: key-auth
            config:
              hide_credentials: false
          - name: acl
            config:
              hide_groups_header: true
              allow:
                - admin
      - name: dashboard
        _comment: 'Studio: /* -> http://release-name-supabase-studio:3000/*'
        url: http://release-name-supabase-studio:3000/
        routes:
          - name: dashboard-all
            strip_path: true
            paths:
              - /
        plugins:
          - name: cors
          - name: basic-auth
            config:
              hide_credentials: true
---
# Source: enkryptai-stack/charts/supabase/templates/minio/volume.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-pvc
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "50Gi"
---
# Source: enkryptai-stack/charts/supabase/templates/storage/volume.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: supabase-storage-pvc
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  storageClassName: gp3-encrypted
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "20Gi"
---
# Source: enkryptai-stack/charts/openfga/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: openfga-job-status-reader
rules:
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - get
  - list
---
# Source: enkryptai-stack/charts/openfga/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: openfga-job-status-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: openfga-job-status-reader
subjects:
- kind: ServiceAccount
  name: openfga
---
# Source: enkryptai-stack/charts/frontend/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    helm.sh/chart: frontend-1.0.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
---
# Source: enkryptai-stack/charts/gateway-kong/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway-kong
  labels:
    helm.sh/chart: gateway-kong-1.0.0
    app.kubernetes.io/name: gateway-kong
    app.kubernetes.io/instance: release-name
    app: gateway-kong
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: gateway-kong
    app.kubernetes.io/instance: release-name
    app: gateway-kong
  ports:
    - name: http
      port: 80
      targetPort: 8000
    - name: https
      port: 443
      targetPort: 8443
    - name: admin
      port: 8001
      targetPort: 8001
    - name: admin-gui
      port: 8002
      targetPort: 8002
    - name: fluent-bit-metrics
      port: 2020
      targetPort: 2020
    - name: fluent-bit-admin
      port: 5043
      targetPort: 5043
---
# Source: enkryptai-stack/charts/guardrails/templates/local-model/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: guardrails-model-svc
  labels:
    app: guardrails-model
spec:
  type: ClusterIP
  selector:
    app: guardrails-model
  ports:
    - name: nsfw
      port: 8001
      targetPort: 8001
      protocol: TCP
    - name: toxicity
      port: 8002
      targetPort: 8002
      protocol: TCP
    - name: promptinject
      port: 8003
      targetPort: 8003
      protocol: TCP
    - name: pii
      port: 8004
      targetPort: 8004
      protocol: TCP
    - name: llm
      port: 8005
      targetPort: 8005
      protocol: TCP
    - name: language
      port: 8006
      targetPort: 8006
      protocol: TCP
    - name: router
      port: 8080
      targetPort: 8080
      protocol: TCP
---
# Source: enkryptai-stack/charts/guardrails/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: guardrails
  labels:
    helm.sh/chart: guardrails-1.0.0
    app.kubernetes.io/name: guardrails
    app.kubernetes.io/instance: release-name
    app: guardrails
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: guardrails
    app.kubernetes.io/instance: release-name
    app: guardrails
---
# Source: enkryptai-stack/charts/litellm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: litellm
  labels:
    helm.sh/chart: litellm-1.0.0
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 443
      targetPort: http
      protocol: TCP
      name: https
  selector:
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/openfga/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: openfga
  labels:
    helm.sh/chart: openfga-0.2.26
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.8.9"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 8081
      targetPort: grpc
      protocol: TCP
    - name: http 
      port: 8080
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 2112
      targetPort: metrics
      protocol: TCP

  selector:
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/redteam-proxy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redteam-proxy
  labels:
    helm.sh/chart: redteam-proxy-1.0.0
    app.kubernetes.io/name: redteam-proxy
    app.kubernetes.io/instance: release-name
    app: redteam-proxy
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9091
      targetPort: 9091
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: redteam-proxy
    app.kubernetes.io/instance: release-name
    app: redteam-proxy
---
# Source: enkryptai-stack/charts/supabase/templates/auth/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-supabase-auth
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9999
      targetPort: 9999
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-auth
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/kong/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-supabase-kong
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-kong
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/meta/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-supabase-meta
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-meta
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/minio/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-minio
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/rest/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-supabase-rest
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-rest
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/storage/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: supabase-storage
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5000
      targetPort: 5000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-storage
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/supabase/templates/studio/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-supabase-studio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: supabase-studio
    app.kubernetes.io/instance: release-name
---
# Source: enkryptai-stack/charts/frontend/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  annotations:
    instrumentation.opentelemetry.io/inject-nodejs: "true"
    reloader.stakater.com/auto: "true"
  labels:
    helm.sh/chart: frontend-1.0.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: frontend
      app.kubernetes.io/instance: release-name
      app: frontend
  template:
    metadata:
      annotations:
        instrumentation.opentelemetry.io/inject-nodejs: "true"
        reloader.stakater.com/auto: "true"
      labels:
        helm.sh/chart: frontend-1.0.0
        app.kubernetes.io/name: frontend
        app.kubernetes.io/instance: release-name
        app: frontend
        app.kubernetes.io/version: "dev"
        app.kubernetes.io/managed-by: Helm
        app: frontend
    spec:
      
      containers:
        - name: frontend
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/frontend:7961f95"
          imagePullPolicy: IfNotPresent
          env:
            - name: NODE_TLS_REJECT_UNAUTHORIZED
              value: "0"
            - name: NEXT_PUBLIC_URL
              valueFrom:
                configMapKeyRef:
                  name: frontend
                  key: NEXT_PUBLIC_URL
          envFrom:
            - secretRef:
                name: frontend-env-secret
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1700m
              memory: 4Gi
            requests:
              cpu: 200m
              memory: 512Mi
---
# Source: enkryptai-stack/charts/gateway-kong/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-kong
  labels:
    helm.sh/chart: gateway-kong-1.0.0
    app.kubernetes.io/name: gateway-kong
    app.kubernetes.io/instance: release-name
    app: gateway-kong
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: gateway-kong
      app.kubernetes.io/instance: release-name
      app: gateway-kong
  template:
    metadata:
      labels:
        helm.sh/chart: gateway-kong-1.0.0
        app.kubernetes.io/name: gateway-kong
        app.kubernetes.io/instance: release-name
        app: gateway-kong
        app.kubernetes.io/version: "dev"
        app.kubernetes.io/managed-by: Helm
    spec:
      
      initContainers:
        - name: gateway-migrations
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/gateway:dev-1767798930"
          command:
            - kong
            - migrations
            - bootstrap
          envFrom:
            - secretRef:
                name: gateway-migration-env-secret
        - name: gateway-migrations-up
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/gateway:dev-1767798930"
          command:
            - /bin/sh
            - -c
            - kong migrations up && kong migrations finish
          envFrom:
            - secretRef:
                name: gateway-migration-env-secret
      containers:
        - name: gateway
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/gateway:dev-1767798930"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: gateway-env-secret
          livenessProbe:
            exec:
              command:
                - kong
                - health
            failureThreshold: 10
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          ports:
            - containerPort: 8000
              protocol: TCP
            - containerPort: 8443
              protocol: TCP
            - containerPort: 8001
              protocol: TCP
            - containerPort: 8002
              protocol: TCP
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "0.5"
              memory: 1.5Gi
          securityContext:
            allowPrivilegeEscalation: true
          volumeMounts:
            - mountPath: /var/run/kong
              name: kong-prefix-vol
            - mountPath: /tmp
              name: kong-tmp-vol
        - name: gateway-deck-sync
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/gateway-sync:a8a9b2c"
          envFrom:
            - secretRef:
                name: gateway-env-secret
          command:
            - /bin/sh
          args:
            - -c
            - |
              echo "Start Waiting for Kong Admin API to be ready.."
              until curl -s http://localhost:8001/status > /dev/null; do
                echo "Waiting for Kong Admin API to be ready..."
                sleep 5
              done

              /mnt/deck/kong-deck.sh

              # Keep the container running after sync
              while true; do
                sleep 3600  # Sleep for 1 hour
                #  we can add periodic sync here
                /mnt/deck/kong-deck.sh
              done
          resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.1"
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: true
            runAsUser: 0
          volumeMounts:
            - mountPath: /mnt/deck
              name: gateway-kong-temp-config
        - name: fluent-bit
          image: "vpcdepoyment.azurecr.io/onprem/fluent-bit:3.2.1"
          env:
            
            - name: ELASTIC_HOST
              valueFrom:
                secretKeyRef:
                  key: DECK_ELASTIC_HOST
                  name: gateway-env-secret
            - name: ELASTIC_PORT
              value: "9200"
            - name: ELASTIC_USERNAME
              valueFrom:
                secretKeyRef:
                  key: username
                  name: enkryptai-opensearch-admin-password
            - name: ELASTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: enkryptai-opensearch-admin-password
          ports:
            - containerPort: 2020
              protocol: TCP
            - containerPort: 5043
              protocol: TCP
            - containerPort: 5044
              protocol: TCP
            - containerPort: 5045
              protocol: TCP
            - containerPort: 5046
              protocol: TCP
            - containerPort: 5047
              protocol: TCP
          volumeMounts:
            - mountPath: /fluent-bit/etc/
              name: fluent-bit-config
            - mountPath: /tmp/storage
              name: fluent-bit-storage
            - mountPath: /var/log
              name: varlog
            - mountPath: /var/lib/docker/containers
              name: varlibdockercontainers
              readOnly: true
          resources:
            limits:
              cpu: "0.5"
              memory: 256Gi
            requests:
              cpu: "0.1"
              memory: 64Mi
      volumes:
        - configMap:
            defaultMode: 511
            name: gateway-kong-temp-config
          name: gateway-kong-temp-config
        - name: kong-prefix-vol
          emptyDir: {}
        - name: kong-tmp-vol
          emptyDir: {}
        - configMap:
            defaultMode: 420
            name: fluent-bit-config
          name: fluent-bit-config
        - emptyDir: {}
          name: fluent-bit-storage
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
---
# Source: enkryptai-stack/charts/guardrails/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: guardrails
  annotations:
    reloader.stakater.com/auto: "true"
  labels:
    helm.sh/chart: guardrails-1.0.0
    app.kubernetes.io/name: guardrails
    app.kubernetes.io/instance: release-name
    app: guardrails
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: guardrails
      app.kubernetes.io/instance: release-name
      app: guardrails
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: guardrails-1.0.0
        app.kubernetes.io/name: guardrails
        app.kubernetes.io/instance: release-name
        app: guardrails
        app.kubernetes.io/version: "dev"
        app.kubernetes.io/managed-by: Helm
        app: guardrails
    spec:
      
      containers:
        - name: guardrails
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/guardrails:prodgr-v0.0.5"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          envFrom:
            - secretRef:
                name: guardrails-env-secret
          env:
            - name: LOCAL_GUARDRAILS
              value: "true"
          livenessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 90
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 120
            timeoutSeconds: 30
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              cpu: 4
              memory: 16Gi
              nvidia.com/gpu: 1
---
# Source: enkryptai-stack/charts/guardrails/templates/local-model/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: guardrails-model
  labels:
    app: guardrails-model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: guardrails-model
  template:
    metadata:
      labels:
        app: guardrails-model
    spec:
      runtimeClassName: nvidia
      tolerations:
        - effect: NoSchedule
          key: instance
          operator: Equal
          value: llm-model-guardrail

      containers:
        - name: guardrails-model
          image: "/enkryptai-dev/guardrail-model:02d81dc"
          imagePullPolicy: IfNotPresent

          ports:
            - name: nsfw
              containerPort: 8001
            - name: toxicity
              containerPort: 8002
            - name: promptinject
              containerPort: 8003
            - name: pii
              containerPort: 8004
            - name: llm
              containerPort: 8005
            - name: language
              containerPort: 8006
            - name: router
              containerPort: 8080

          resources:
            limits:
              cpu: 4
              memory: 8Gi
            requests:
              cpu: 2
              memory: 4Gi
              nvidia.com/gpu: 1
          envFrom:
            - secretRef:
                name: guardrails-model-secret
---
# Source: enkryptai-stack/charts/litellm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm
  labels:
    helm.sh/chart: litellm-1.0.0
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: litellm
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        secret.reloader.stakater.com/reload: litellm-gateway-env-secret
      labels:
        helm.sh/chart: litellm-1.0.0
        app.kubernetes.io/name: litellm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "1.0"
        app.kubernetes.io/managed-by: Helm
        app: litellm-gateway
    spec:
      containers:
        - name: litellm
          image: "vpcdepoyment.azurecr.io/enkryptai-dev/litellm-gateway:dev"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 443
              protocol: TCP
---
# Source: enkryptai-stack/charts/openfga/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: openfga
  labels:
    helm.sh/chart: openfga-0.2.26
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.8.9"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: openfga
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: /metrics
        prometheus.io/port: "2112"
      labels:
        app.kubernetes.io/name: openfga
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: openfga
      securityContext:
        {}
      
      initContainers:
        - name: wait-for-migration
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/k8s-wait-for:v2.0"
          imagePullPolicy: IfNotPresent
          args: ["job-wr", 'openfga-migrate']
          resources:
            {}
      containers:
        - name: openfga
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/openfga:v1.8.9"
          imagePullPolicy: Always
          args: ["run"]
          ports:
            - name: grpc
              containerPort: 8081

            - name: http
              containerPort: 8080
              protocol: TCP

            - name: profiler
              containerPort: 3001
              protocol: TCP

            - name: playground
              containerPort: 3000
              protocol: TCP
            - name: metrics
              containerPort: 2112
              protocol: TCP

          env:
            - name: OPENFGA_DATASTORE_ENGINE
              value: "postgres"
            - name: OPENFGA_DATASTORE_URI
              valueFrom:
                secretKeyRef:
                  name: "openfga-env-secret"
                  key: "uri"
            - name: OPENFGA_DATASTORE_MAX_CACHE_SIZE
              value: "100000"
            - name: OPENFGA_DATASTORE_MAX_OPEN_CONNS
              value: "70"
            - name: OPENFGA_DATASTORE_MAX_IDLE_CONNS
              value: "30"
            - name: OPENFGA_DATASTORE_CONN_MAX_IDLE_TIME
              value: "1m"
            - name: OPENFGA_DATASTORE_CONN_MAX_LIFETIME
              value: "1h"
            - name: OPENFGA_GRPC_ADDR
              value: "0.0.0.0:8081"

            - name: OPENFGA_HTTP_ENABLED
              value: "true"
            - name: OPENFGA_HTTP_ADDR
              value: "0.0.0.0:8080"
            - name: OPENFGA_HTTP_CORS_ALLOWED_ORIGINS
              value: "*"
            - name: OPENFGA_HTTP_CORS_ALLOWED_HEADERS
              value: "*"
            - name: OPENFGA_AUTHN_METHOD
              value: none

            - name: OPENFGA_PLAYGROUND_ENABLED
              value: "false"
            - name: OPENFGA_PLAYGROUND_PORT
              value: "3000"
            - name: OPENFGA_LOG_FORMAT
              value: json
            - name: OPENFGA_LOG_LEVEL
              value: info
            - name: OPENFGA_LOG_TIMESTAMP_FORMAT
              value: Unix
            - name: OPENFGA_LIST_OBJECTS_DEADLINE
              value: "3s"
            - name: OPENFGA_LIST_OBJECTS_MAX_RESULTS
              value: "1000"
            - name: OPENFGA_LIST_USERS_DEADLINE
              value: "3s"
            - name: OPENFGA_LIST_USERS_MAX_RESULTS
              value: "1000"
            - name: OPENFGA_CHECK_QUERY_CACHE_ENABLED
              value: "true"
            - name: OPENFGA_CHECK_QUERY_CACHE_LIMIT
              value: "10000"
            - name: OPENFGA_CHECK_QUERY_CACHE_TTL
              value: "1m"
            - name: OPENFGA_REQUEST_DURATION_DATASTORE_QUERY_COUNT_BUCKETS
              value: "50,200"

            - name: OPENFGA_METRICS_ENABLED
              value: "true"
            - name: OPENFGA_METRICS_ADDR
              value: "0.0.0.0:2112"
            - name: OPENFGA_TRACE_SAMPLE_RATIO
              value: "0.3"
            - name: OPENFGA_HTTP_TLS_ENABLED
              value: "false"
            - name: OPENFGA_REQUEST_TIMEOUT
              value: 3s
            - name: OPENFGA_DATASTORE_METRICS_ENABLED
              value: "true"
            - name: OPENFGA_CHECK_CACHE_LIMIT
              value: "10000"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_ENABLED
              value: "true"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_MAX_RESULTS
              value: "10000"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_TTL
              value: 1m
            - name: OPENFGA_CACHE_CONTROLLER_ENABLED
              value: "true"
            - name: OPENFGA_CACHE_CONTROLLER_TTL
              value: 30s
            - name: OPENFGA_CHECK_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_LIST_OBJECTS_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_LIST_USERS_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_TRACING_ENABLED
              value: "false"
            - name: OPENFGA_TRACE_SERVICE_NAME
              value: openfga
            - name: OPENFGA_ACCESS_CONTROL_ENABLED
              value: "false"
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            grpc:
              port: 8081

          resources:
            {}
---
# Source: enkryptai-stack/charts/redteam-proxy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redteam-proxy
  annotations:
    reloader.stakater.com/auto: "true"
  labels:
    helm.sh/chart: redteam-proxy-1.0.0
    app.kubernetes.io/name: redteam-proxy
    app.kubernetes.io/instance: release-name
    app: redteam-proxy
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: redteam-proxy
      app.kubernetes.io/instance: release-name
      app: redteam-proxy
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 50%
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: redteam-proxy-1.0.0
        app.kubernetes.io/name: redteam-proxy
        app.kubernetes.io/instance: release-name
        app: redteam-proxy
        app.kubernetes.io/version: "dev"
        app.kubernetes.io/managed-by: Helm
        app: redteam-proxy
    spec:
      
      containers:
        - name: redteam-proxy
          image: "vpcdepoyment.azurecr.io/enkryptai-prod/redteam-proxy:prod-v1.0.34"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 9091
              protocol: TCP
          envFrom:
            - secretRef:
                name: redteam-proxy-env-secret
          env:
            - name: NATS_URL
              value: nats://nats:4222
            - name: IS_PROXY_MODE
              value: "true"
          livenessProbe:
            httpGet:
              path: /health
              port: 9091
            initialDelaySeconds: 90
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 9091
            initialDelaySeconds: 30
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 300Mi
---
# Source: enkryptai-stack/charts/supabase/templates/auth/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-supabase-auth
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-auth
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-auth
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-supabase-auth
      securityContext:
        {}
      initContainers:
        - name: init-db
          image: vpcdepoyment.azurecr.io/onprem/postgres:15-alpine
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          command: ["/bin/sh", "-c"]
          args:
            - |
              until pg_isready -h $(DB_HOST) -p $(DB_PORT) -U $(DB_USER); do
              echo "Waiting for database to start..."
              sleep 2
              done
            - echo "Database is ready"
      containers:
        - name: supabase-auth
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/gotrue:v2.169.0"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE
              value: "/auth/v1/verify"
            - name: DB_PASSWORD_ENC
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: GOTRUE_DB_DATABASE_URL
              value: $(DB_DRIVER)://$(DB_USER):$(DB_PASSWORD_ENC)@$(DB_HOST):$(DB_PORT)/$(DB_NAME)?search_path=auth&sslmode=$(DB_SSL)
            - name: GOTRUE_DB_DRIVER
              value: $(DB_DRIVER)
            - name: GOTRUE_JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: secretjwt
            - name: GOTRUE_SMTP_USER
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: smtpusername
            - name: GOTRUE_SMTP_PASS
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: smtppassword
          ports:
            - name: http
              containerPort: 9999
              protocol: TCP
---
# Source: enkryptai-stack/charts/supabase/templates/kong/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-supabase-kong
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-kong
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-kong
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-supabase-kong
      securityContext:
        {}
      containers:
        - name: supabase-kong
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/kong:2.8.1"
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash"]
          args: ["/scripts/wrapper.sh"]
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: KONG_DATABASE
              value: "off"
            - name: KONG_DECLARATIVE_CONFIG
              value: "/usr/local/kong/kong.yml"
            - name: KONG_DNS_ORDER
              value: "LAST,A,CNAME"
            - name: KONG_LOG_LEVEL
              value: "warn"
            - name: KONG_NGINX_PROXY_PROXY_BUFFERS
              value: "64 160k"
            - name: KONG_NGINX_PROXY_PROXY_BUFFER_SIZE
              value: "160k"
            - name: KONG_PLUGINS
              value: "request-transformer,cors,key-auth,acl,basic-auth"
            - name: SUPABASE_ANON_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: anonKey
            - name: SUPABASE_SERVICE_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: serviceKey
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          volumeMounts:
            - mountPath: /usr/local/kong/template.yml
              name: config
              subPath: template.yml
            - mountPath: /scripts
              name: wrapper
      volumes:
        - name: config
          configMap:
            name: release-name-supabase-kong
            defaultMode: 0777
            items:
            - key: template.yml
              path: template.yml
        - name: wrapper
          configMap:
            name: release-name-supabase-kong
            defaultMode: 0777
            items:
            - key: wrapper.sh
              path: wrapper.sh
---
# Source: enkryptai-stack/charts/supabase/templates/meta/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-supabase-meta
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-meta
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-meta
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-supabase-meta
      securityContext:
        {}
      containers:
        - name: supabase-meta
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/postgres-meta:v0.86.0"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: PG_META_PORT
              value: "8080"
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbdatabase
            - name: PG_META_DB_HOST
              value: $(DB_HOST)
            - name: PG_META_DB_PORT
              value: $(DB_PORT)
            - name: PG_META_DB_NAME
              value: $(DB_NAME)
            - name: PG_META_DB_USER
              value: $(DB_USER)
            - name: PG_META_DB_PASSWORD
              value: $(DB_PASSWORD)
            - name: PG_META_DB_SSL_MODE
              value: $(DB_SSL)
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
---
# Source: enkryptai-stack/charts/supabase/templates/rest/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-supabase-rest
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-rest
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-rest
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-supabase-rest
      securityContext:
        {}
      containers:
        - name: supabase-rest
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/postgrest:v12.2.8"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: PGRST_APP_SETTINGS_JWT_EXP
              value: "3600"
            - name: PGRST_DB_ANON_ROLE
              value: "anon"
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: DB_PASSWORD_ENC
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbdatabase
            - name: PGRST_DB_URI
              value: $(DB_DRIVER)://$(DB_USER):$(DB_PASSWORD_ENC)@$(DB_HOST):$(DB_PORT)/$(DB_NAME)?sslmode=$(DB_SSL)
            - name: PGRST_JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: secretjwt
            - name: PGRST_APP_SETTINGS_JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: secretjwt
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
---
# Source: enkryptai-stack/charts/supabase/templates/studio/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-supabase-studio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-studio
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install
        helm.sh/hook-weight: "5"
      labels:
        app.kubernetes.io/name: supabase-studio
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-supabase-studio
      securityContext:
        {}
      containers:
        - name: supabase-studio
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/studio:20240326-5e5586d"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: SUPABASE_TYPE
              value: "studio"
            - name: SUPABASE_URL
              value: http://release-name-supabase-kong:8000
            - name: STUDIO_PG_META_URL
              value: http://release-name-supabase-meta:8080
            - name: SUPABASE_ANON_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: anonKey
            - name: SUPABASE_SERVICE_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: serviceKey
          livenessProbe:
            httpGet:
              path: /api/profile
              port: 3000
            initialDelaySeconds: 3
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
---
# Source: enkryptai-stack/charts/frontend/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend
  labels:
    helm.sh/chart: frontend-1.0.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: enkryptai-stack/charts/gateway-kong/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gateway-kong
  labels:
    helm.sh/chart: gateway-kong-1.0.0
    app.kubernetes.io/name: gateway-kong
    app.kubernetes.io/instance: release-name
    app: gateway-kong
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gateway-kong
  minReplicas: 1
  maxReplicas: 2
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: enkryptai-stack/charts/guardrails/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: guardrails
  labels:
    helm.sh/chart: guardrails-1.0.0
    app.kubernetes.io/name: guardrails
    app.kubernetes.io/instance: release-name
    app: guardrails
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: guardrails
  minReplicas: 1
  maxReplicas: 2
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: enkryptai-stack/charts/redteam-proxy/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: redteam-proxy
  labels:
    helm.sh/chart: redteam-proxy-1.0.0
    app.kubernetes.io/name: redteam-proxy
    app.kubernetes.io/instance: release-name
    app: redteam-proxy
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: redteam-proxy
  minReplicas: 2
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
---
# Source: enkryptai-stack/charts/supabase/templates/minio/deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: minio
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-minio
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-minio
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: minio
      securityContext:
        {}
      containers:
        - name: supabase-minio
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/minio:latest"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          args:
            - server
            - --console-address
            - ":9001"
            - /data
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: console
              containerPort: 9001
              protocol: TCP
          volumeMounts:
            - mountPath: /data
              name: minio-data
  volumeClaimTemplates:
    - metadata:
        name: minio-data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
---
# Source: enkryptai-stack/charts/supabase/templates/storage/deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: supabase-storage
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: supabase-storage
  selector:
    matchLabels:
      app.kubernetes.io/name: supabase-storage
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade
        helm.sh/hook-weight: "15"
      labels:
        app.kubernetes.io/name: supabase-storage
        app.kubernetes.io/instance: release-name
    spec:
      restartPolicy: Always
      serviceAccountName: supabase-storage
      securityContext:
        {}
      initContainers:
        - name: init-db
          image: vpcdepoyment.azurecr.io/onprem/postgres:15-alpine
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          command: ["/bin/sh", "-c"]
          args:
            - |
              echo $(DB_HOST)
              echo $(DB_PORT) 
              echo $(DB_USER) 
              until pg_isready -h $(DB_HOST) -p $(DB_PORT) -U $(DB_USER); do
              echo "Waiting for database to start..."
              sleep 2
              done
            - echo "Database is ready"
        
        - env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: MINIO_ROOT_USER
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: MINIO_ROOT_PASSWORD
            - name: S3_PROTOCOL_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: S3_PROTOCOL_ACCESS_KEY_ID
            - name: S3_PROTOCOL_ACCESS_KEY_SECRET
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: S3_PROTOCOL_ACCESS_KEY_SECRET
          name: init-bucket
          image: vpcdepoyment.azurecr.io/onprem/mc:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              until /usr/bin/mc alias set supa-minio http://minio:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD; do
                echo "Waiting for minio to start..."
                sleep 2
              done
              /usr/bin/mc mb --ignore-existing supa-minio/stub
              /usr/bin/mc admin user add supa-minio $S3_PROTOCOL_ACCESS_KEY_ID $S3_PROTOCOL_ACCESS_KEY_SECRET
              /usr/bin/mc admin policy attach supa-minio readwrite --user=$S3_PROTOCOL_ACCESS_KEY_ID
              echo "User $S3_PROTOCOL_ACCESS_KEY_ID created and readwrite policy attached."
      containers:
        - name: supabase-storage
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/storage-api:v1.19.1"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: onprem
          env:
            - name: AWS_DEFAULT_REGION
              value: "stub"
            - name: FILE_SIZE_LIMIT
              value: "52428800"
            - name: FILE_STORAGE_BACKEND_PATH
              value: "/var/lib/storage"
            - name: GLOBAL_S3_BUCKET
              value: "stub"
            - name: GLOBAL_S3_ENDPOINT
              value: "http://minio:9000"
            - name: GLOBAL_S3_FORCE_PATH_STYLE
              value: "true"
            - name: GLOBAL_S3_PROTOCOL
              value: "http"
            - name: PGOPTIONS
              value: "-c search_path=storage,public"
            - name: REGION
              value: "stub"
            - name: STORAGE_BACKEND
              value: "s3"
            - name: POSTGREST_URL
              value: http://release-name-supabase-rest:3000
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: DB_PASSWORD_ENC
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbpassword
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: dbdatabase
            - name: DATABASE_URL
              value: $(DB_DRIVER)://$(DB_USER):$(DB_PASSWORD_ENC)@$(DB_HOST):$(DB_PORT)/$(DB_NAME)?search_path=auth&sslmode=$(DB_SSL)
            - name: PGRST_JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: secretjwt
            - name: ANON_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: anonKey
            - name: SERVICE_KEY
              valueFrom:
                secretKeyRef:
                  name: onprem
                  key: serviceKey
            - name: GLOBAL_S3_ENDPOINT
              value: http://minio:9000
          ports:
            - name: http
              containerPort: 5000
              protocol: TCP
          volumeMounts:
            - mountPath: /var/lib/storage
              name: storage-data
  volumeClaimTemplates:
    - metadata:
        name: storage-data
      spec:
        accessModes:
          - ReadWriteOnce
        storageClassName: 
        resources:
          requests:
            storage: 20Gi
---
# Source: enkryptai-stack/charts/frontend/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  labels:
    helm.sh/chart: frontend-1.0.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: release-name
    app: frontend
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-buffer-size: 200m
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "app.anishs.xyz"
      secretName: custom-cert-front
  rules:
    - host: "app.anishs.xyz"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 3000
---
# Source: enkryptai-stack/charts/gateway-kong/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gateway-kong
  labels:
    helm.sh/chart: gateway-kong-1.0.0
    app.kubernetes.io/name: gateway-kong
    app.kubernetes.io/instance: release-name
    app: gateway-kong
    app.kubernetes.io/version: "dev"
    app.kubernetes.io/managed-by: Helm
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-buffer-size: 200m
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "api.anishs.xyz"
      secretName: custom-cert-front
  rules:
    - host: "api.anishs.xyz"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: gateway-kong
                port:
                  number: 80
---
# Source: enkryptai-stack/charts/supabase/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-supabase
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-buffer-size: 200m
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "auth.anishs.xyz"
      secretName: custom-cert-front
  rules:
    - host: "auth.anishs.xyz"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: release-name-supabase-kong
                port:
                  number: 8000
---
# Source: enkryptai-stack/charts/openfga/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "openfga-test-connection"
  labels:
    helm.sh/chart: openfga-0.2.26
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.8.9"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: grpc-health-probe
      image: "vpcdepoyment.azurecr.io/onprem/openfga:v1.8.9"
      imagePullPolicy: Always
      command: ["grpc_health_probe", '-addr=openfga:8081']
  restartPolicy: Never
---
# Source: enkryptai-stack/charts/openfga/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: openfga-migrate
  labels:
    helm.sh/chart: openfga-0.2.26
    app.kubernetes.io/name: openfga
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.8.9"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback, post-delete
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "-5"
spec:
  template:
    metadata:
      annotations:
        helm.sh/hook: post-install, post-upgrade, post-rollback, post-delete
        helm.sh/hook-delete-policy: before-hook-creation
        helm.sh/hook-weight: "-5"
    spec:
      serviceAccountName: openfga
      containers:
        - name: migrate-database
          securityContext:
            {}
          image: "vpcdepoyment.azurecr.io/onprem/openfga:v1.8.9"
          args: ["migrate"]
          env:
            - name: OPENFGA_DATASTORE_ENGINE
              value: "postgres"
            - name: OPENFGA_DATASTORE_URI
              valueFrom:
                secretKeyRef:
                  name: "openfga-env-secret"
                  key: "uri"
            - name: OPENFGA_HTTP_TLS_ENABLED
              value: "false"
            - name: OPENFGA_REQUEST_TIMEOUT
              value: 3s
            - name: OPENFGA_DATASTORE_METRICS_ENABLED
              value: "true"
            - name: OPENFGA_CHECK_CACHE_LIMIT
              value: "10000"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_ENABLED
              value: "true"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_MAX_RESULTS
              value: "10000"
            - name: OPENFGA_CHECK_ITERATOR_CACHE_TTL
              value: 1m
            - name: OPENFGA_CACHE_CONTROLLER_ENABLED
              value: "true"
            - name: OPENFGA_CACHE_CONTROLLER_TTL
              value: 30s
            - name: OPENFGA_CHECK_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_LIST_OBJECTS_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_LIST_USERS_DISPATCH_THROTTLING_ENABLED
              value: "false"
            - name: OPENFGA_TRACING_ENABLED
              value: "false"
            - name: OPENFGA_TRACE_SERVICE_NAME
              value: openfga
            - name: OPENFGA_ACCESS_CONTROL_ENABLED
              value: "false"

          resources:
            {}
      restartPolicy: Never
  backoffLimit: 1
---
# Source: enkryptai-stack/charts/supabase/templates/s3-bucket.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: create-s3-buckets
  annotations:
    "helm.sh/hook": post-install, post-upgrade
spec:
  template:
    spec:
      containers:
      - name: create-buckets
        image: vpcdepoyment.azurecr.io/onprem/aws-cli:2.27.55
        command:
        - /bin/sh
        - -c
        - |
          set -e

          ENDPOINT="http://supabase-storage.enkryptai-stack.svc:5000/s3"
          HEALTH_URL="http://supabase-storage.enkryptai-stack.svc:5000/health"

          echo " Checking Supabase Storage health endpoint with service role token..."
          until [ "$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" $HEALTH_URL)" -eq 200 ]; do
            echo " Storage not healthy yet. Sleeping 10s..."
            sleep 10
          done

          echo " Storage service is healthy. Proceeding to bucket creation."

          BUCKETS="
          redteam
          red_teaming
          playground
          datasets
          dataset_red_teaming
          leaderboard
          code-of-conduct
          redteam_reports
          "

          for bucket in $BUCKETS; do
            if aws s3 ls "s3://$bucket" --endpoint-url $ENDPOINT >/dev/null 2>&1; then
              echo " Bucket '$bucket' already exists. Skipping..."
            else
              echo " Creating bucket '$bucket'..."
              aws s3 mb "s3://$bucket" --endpoint-url $ENDPOINT
            fi
          done

          echo " Bucket creation process completed."
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_DEFAULT_REGION  
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: AWS_DEFAULT_REGION
        - name: SUPABASE_SERVICE_ROLE_KEY
          valueFrom:
            secretKeyRef:
              name: onprem
              key: serviceKey
      restartPolicy: Never
  backoffLimit: 1
---
# Source: enkryptai-stack/charts/supabase/templates/test/auth.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-auth
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-auth
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://release-name-supabase-auth:9999/health
              echo "Sevice release-name-supabase-auth is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/kong.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-kong
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - envFrom:
            - secretRef:
                name: onprem
          name: test-kong
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              echo "Attempting to access dashboard with provided credentials..."
              curl -sL --fail \
                -o /dev/null \
                "http://${DASHBOARD_USERNAME}:${DASHBOARD_PASSWORD}@release-name-supabase-kong:8000" \
                || ( echo -e "\e[0;31mFailed to get a valid response." && exit 1 )
              echo "Successfully connected."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/meta.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-meta
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-meta
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://release-name-supabase-meta:8080/health
              echo "Sevice release-name-supabase-meta is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/minio.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-minio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-minio
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://minio:9000/minio/health/live
              echo "Sevice minio is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/rest.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-rest
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-rest
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://release-name-supabase-rest:3000
              echo "Sevice release-name-supabase-rest is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/storage.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-storage
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-storage
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://supabase-storage:5000/status
              echo "Sevice supabase-storage is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/charts/supabase/templates/test/studio.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-supabase-test-studio
  labels:
    helm.sh/chart: supabase-0.1.3
    app.kubernetes.io/name: supabase
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
        - name: test-studio
          image: vpcdepoyment.azurecr.io/onprem/curljq:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - |
              curl -sfo /dev/null \
                http://release-name-supabase-studio:3000/api/profile
              echo "Sevice release-name-supabase-studio is healthy."
      restartPolicy: Never
---
# Source: enkryptai-stack/templates/openfga/openfga-sync-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: openfga-sync
  annotations:
    "helm.sh/hook-weight": "-6"
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  backoffLimit: 5
  template:
    spec:
      restartPolicy: Never
      initContainers:
        - name: wait-for-openfga
          image: vpcdepoyment.azurecr.io/onprem/busybox:latest
          command: ["sh", "-c", "until wget -q --spider http://openfga:8080/healthz; do echo 'Waiting for OpenFGA...'; sleep 2; done"]
      containers:
        - name: openfga-sync
          image: vpcdepoyment.azurecr.io/enkryptai-prod/openfga-sync:v1.3.0
          env:
            - name: FGA_API_URL
              value: "http://openfga:8080"
            - name: FGA_MODEL_ID
              value: "latest"
